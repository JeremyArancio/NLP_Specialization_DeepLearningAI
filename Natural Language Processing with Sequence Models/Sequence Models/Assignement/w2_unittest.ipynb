{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import trax\n",
    "import pickle\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_line_to_tensor(target):\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"simple_test_case1\",\n",
    "            \"input\": {\"line\": \"abc xyz\", \"EOS_int\": 1},\n",
    "            \"expected\": [97, 98, 99, 32, 120, 121, 122, 1],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"simple_test_case2\",\n",
    "            \"input\": {\"line\": \"abc xyz\", \"EOS_int\": -1},\n",
    "            \"expected\": [97, 98, 99, 32, 120, 121, 122, -1],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"simple_test_case3\",\n",
    "            \"input\": {\"line\": \"hello world!\"},\n",
    "            \"expected\": [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33, 1],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"simple_test_case4\",\n",
    "            \"input\": {\"line\": \"12345.\", \"EOS_int\": 1},\n",
    "            \"expected\": [49, 50, 51, 52, 53, 46, 1],\n",
    "        },\n",
    "        {\"name\": \"simple_test_case5\", \"input\": {\"line\": \"\"}, \"expected\": [1]},\n",
    "        {\n",
    "            \"name\": \"simple_test_case6\",\n",
    "            \"input\": {\"line\": \"\", \"EOS_int\": -10},\n",
    "            \"expected\": [-10],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"simple_test_case6\",\n",
    "            \"input\": {\"line\": \" \", \"EOS_int\": 1},\n",
    "            \"expected\": [32, 1],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert isinstance(result, list)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": type(test_case[\"expected\"]),\n",
    "                    \"got\": type(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Output from line_to_tensor is not of type list.\\n\\tExpected {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert len(result) == len(test_case[\"expected\"])\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": len(test_case[\"expected\"]),\n",
    "                    \"got\": len(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Output from line_to_tensor has not the correct number of elements.\\n\\tExpected {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert result == test_case[\"expected\"]\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Output from line_to_tensor has not the correct values.\\n\\tExpected {failed_cases[-1].get('expected')}.\\n\\tGot: {failed_cases[-1].get('got')}\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"check_default_example\",\n",
    "            \"input\": {\n",
    "                \"batch_size\": 2,\n",
    "                \"max_length\": 10,\n",
    "                \"data_lines\": [\"12345678901\", \"123456789\", \"234567890\", \"345678901\"],\n",
    "                \"shuffle\": False,\n",
    "            },\n",
    "            \"expected\": (\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1],\n",
    "                        [50, 51, 52, 53, 54, 55, 56, 57, 48, 1],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1],\n",
    "                        [50, 51, 52, 53, 54, 55, 56, 57, 48, 1],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"check_lenghts_batchsize\",\n",
    "            \"input\": {\n",
    "                \"batch_size\": 4,\n",
    "                \"max_length\": 11,\n",
    "                \"data_lines\": [\n",
    "                    \"0123456\",\n",
    "                    \"01234567890\",\n",
    "                    \"123456789\",\n",
    "                    \"345678901\",\n",
    "                    \"5678901\",\n",
    "                    \"234567890\",\n",
    "                ],\n",
    "                \"shuffle\": False,\n",
    "            },\n",
    "            \"expected\": (\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [48, 49, 50, 51, 52, 53, 54, 1, 0, 0, 0],\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1, 0],\n",
    "                        [51, 52, 53, 54, 55, 56, 57, 48, 49, 1, 0],\n",
    "                        [53, 54, 55, 56, 57, 48, 49, 1, 0, 0, 0],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [48, 49, 50, 51, 52, 53, 54, 1, 0, 0, 0],\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1, 0],\n",
    "                        [51, 52, 53, 54, 55, 56, 57, 48, 49, 1, 0],\n",
    "                        [53, 54, 55, 56, 57, 48, 49, 1, 0, 0, 0],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"check_shuffle\",\n",
    "            \"input\": {\n",
    "                \"batch_size\": 3,\n",
    "                \"max_length\": 10,\n",
    "                \"data_lines\": [\n",
    "                    \"0123456\",\n",
    "                    \"01234567890\",\n",
    "                    \"123456789\",\n",
    "                    \"345678901\",\n",
    "                    \"5678901\",\n",
    "                    \"234567890\",\n",
    "                ],\n",
    "                \"shuffle\": True,\n",
    "            },\n",
    "            \"expected\": (\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1],\n",
    "                        [50, 51, 52, 53, 54, 55, 56, 57, 48, 1],\n",
    "                        [51, 52, 53, 54, 55, 56, 57, 48, 49, 1],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [49, 50, 51, 52, 53, 54, 55, 56, 57, 1],\n",
    "                        [50, 51, 52, 53, 54, 55, 56, 57, 48, 1],\n",
    "                        [51, 52, 53, 54, 55, 56, 57, 48, 49, 1],\n",
    "                    ]\n",
    "                ),\n",
    "                jnp.array(\n",
    "                    [\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "\n",
    "        if test_case[\"name\"] == \"check_shuffle\":\n",
    "            rnd.seed(32)\n",
    "\n",
    "        tmp_data_gen = target(**test_case[\"input\"])\n",
    "\n",
    "        tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "        try:\n",
    "            assert len(tmp_batch) == 3\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": \"data_generator_output_size\",\n",
    "                    \"expected\": 3,\n",
    "                    \"got\": len(tmp_batch),\n",
    "                }\n",
    "            )\n",
    "            print(f\"Your data generator is yielding more than 3 objects.\")\n",
    "\n",
    "        # Testing types\n",
    "        for index, elem_batch in enumerate(tmp_batch):\n",
    "            try:\n",
    "                assert isinstance(elem_batch, jax.interpreters.xla.DeviceArray)\n",
    "                successful_cases += 1\n",
    "            except:\n",
    "                failed_cases.append(\n",
    "                    {\n",
    "                        \"name\": \"data_generator_output_types\",\n",
    "                        \"expected\": jax.interpreters.xla.DeviceArray,\n",
    "                        \"got\": type(elem_batch),\n",
    "                    }\n",
    "                )\n",
    "                print(\n",
    "                    f\"Element with index {index} in the output tuple has incorrect type.\\n\\t Expected {failed_cases[-1].get('expected')}.\\n\\tGot {failed_cases[-1].get('got')}.\"\n",
    "                )\n",
    "\n",
    "        # Testing shapes\n",
    "        for index, elem_batch in enumerate(tmp_batch):\n",
    "            try:\n",
    "                assert elem_batch.shape == (\n",
    "                    test_case[\"input\"][\"batch_size\"],\n",
    "                    test_case[\"input\"][\"max_length\"],\n",
    "                )\n",
    "                successful_cases += 1\n",
    "            except:\n",
    "                failed_cases.append(\n",
    "                    {\n",
    "                        \"name\": \"data_generator_output_shapes\",\n",
    "                        \"expected\": (\n",
    "                            test_case[\"input\"][\"batch_size\"],\n",
    "                            test_case[\"input\"][\"max_length\"],\n",
    "                        ),\n",
    "                        \"got\": elem_batch.shape,\n",
    "                    }\n",
    "                )\n",
    "                print(\n",
    "                    f\"Element with index {index} in the output tuple has incorrect shape. It should be (batch_size, max_length).\\n\\t Expected {failed_cases[-1].get('expected')}.\\n\\tGot {failed_cases[-1].get('got')}.\"\n",
    "                )\n",
    "\n",
    "        # Testing values\n",
    "        for index, elem_batch in enumerate(tmp_batch):\n",
    "            try:\n",
    "                assert jnp.allclose(elem_batch, test_case[\"expected\"][index])\n",
    "                successful_cases += 1\n",
    "            except:\n",
    "                failed_cases.append(\n",
    "                    {\n",
    "                        \"name\": \"data_generator_output_values\",\n",
    "                        \"expected\": test_case[\"expected\"][index],\n",
    "                        \"got\": elem_batch,\n",
    "                    }\n",
    "                )\n",
    "                print(\n",
    "                    f\"Element with index {index} in the output tuple has incorrect shape. It should be (batch_size, max_length).\\n\\t Expected {failed_cases[-1].get('expected')}.\\n\\tGot {failed_cases[-1].get('got')}.\"\n",
    "                )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_GRULM(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"check_default_model\",\n",
    "            \"input\": {\n",
    "                \"vocab_size\": 256,\n",
    "                \"d_model\": 512,\n",
    "                \"n_layers\": 2,\n",
    "                \"mode\": \"train\",\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"expected_str\": \"Serial[\\n  Serial[\\n    ShiftRight(1)\\n  ]\\n  Embedding_256_512\\n  GRU_512\\n  GRU_512\\n  Dense_256\\n  LogSoftmax\\n]\",\n",
    "                \"expected_sublayers_types\": [\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.core.Embedding,\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.core.Dense,\n",
    "                    trax.layers.base.PureLayer,\n",
    "                ],\n",
    "                \"expected_type\": trax.layers.combinators.Serial,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"check_small_layer\",\n",
    "            \"input\": {\"vocab_size\": 128, \"d_model\": 8, \"n_layers\": 3, \"mode\": \"train\",},\n",
    "            \"expected\": {\n",
    "                \"expected_str\": \"Serial[\\n  Serial[\\n    ShiftRight(1)\\n  ]\\n  Embedding_128_8\\n  GRU_8\\n  GRU_8\\n  GRU_8\\n  Dense_128\\n  LogSoftmax\\n]\",\n",
    "                \"expected_sublayers_types\": [\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.core.Embedding,\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.combinators.Serial,\n",
    "                    trax.layers.core.Dense,\n",
    "                    trax.layers.base.PureLayer,\n",
    "                ],\n",
    "                \"expected_type\": trax.layers.combinators.Serial,\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        # print(test_case[\"name\"])\n",
    "        classifier = target(**test_case[\"input\"])\n",
    "        proposed = str(classifier)\n",
    "\n",
    "        try:\n",
    "            assert proposed.replace(\" \", \"\") == test_case[\"expected\"][\n",
    "                \"expected_str\"\n",
    "            ].replace(\" \", \"\")\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": \"str_rep_check\",\n",
    "                    \"expected\": test_case[\"expected\"][\"expected_str\"],\n",
    "                    \"got\": proposed,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Wrong model.\\n\\tGot: {failed_cases[-1].get('got')},\\n\\tExpected:\\n%s {failed_cases[-1].get('expected')}\"\n",
    "            )\n",
    "\n",
    "        # Test the output type\n",
    "        try:\n",
    "            assert isinstance(classifier, test_case[\"expected\"][\"expected_type\"])\n",
    "            successful_cases += 1\n",
    "            # Test the number of layers\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": \"object_type_check\",\n",
    "                    \"expected\": test_case[\"expected\"][\"expected_type\"],\n",
    "                    \"got\": type(classifier),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                \"The classifier is not an object of type\",\n",
    "                test_case[\"expected\"][\"expected_type\"],\n",
    "                \"Got: \",\n",
    "                failed_cases[-1].get(\"got\"),\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Test\n",
    "            assert len(classifier.sublayers) == len(\n",
    "                test_case[\"expected\"][\"expected_sublayers_types\"]\n",
    "            )\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": \"num_layers_check\",\n",
    "                    \"expected\": len(test_case[\"expected\"][\"expected_sublayers_types\"]),\n",
    "                    \"got\": len(classifier.sublayers),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"The number of sublayers does not match.\\n\\tGot: {len(classifier.sublayers)}.\\n\\tExpected: {len(test_case['expected']['expected_sublayers_types'])}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            sublayers_type = lambda x: list(map(type, x.sublayers))\n",
    "            classifier_sublayers_type = sublayers_type(classifier)\n",
    "\n",
    "            for i in range(len(test_case[\"expected\"][\"expected_sublayers_types\"])):\n",
    "                assert str(classifier_sublayers_type[i]) == str(\n",
    "                    test_case[\"expected\"][\"expected_sublayers_types\"][i]\n",
    "                )\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": \"sublayers_type_check\",\n",
    "                    \"expected\": test_case[\"expected\"][\"expected_sublayers_types\"],\n",
    "                    \"got\": classifier_sublayers_type,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Classifier sublayers do not have the correct type.\\n\\tExpected: {failed_cases[-1].get('expected')}.\\n\\tGot {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_model(target, model, data_generator):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    input_lines = [\n",
    "        \"a lover's complaint\",\n",
    "        \"from off a hill whose concave womb reworded\",\n",
    "        \"a plaintful story from a sistering vale,\",\n",
    "        \"my spirits to attend this double voice accorded,\",\n",
    "        \"and down i laid to list the sad-tuned tale;\",\n",
    "        \"ere long espied a fickle maid full pale,\",\n",
    "        \"tearing of papers, breaking rings a-twain,\",\n",
    "        \"storming her world with sorrow's wind and rain.\",\n",
    "        \"upon her head a platted hive of straw,\",\n",
    "        \"which fortified her visage from the sun,\",\n",
    "        \"whereon the thought might think sometime it saw\",\n",
    "        \"the carcass of beauty spent and done:\",\n",
    "        \"time had not scythed all that youth begun,\",\n",
    "        \"nor youth all quit; but, spite of heaven's fell rage,\",\n",
    "        \"some beauty peep'd through lattice of sear'd age.\",\n",
    "        \"oft did she heave her napkin to her eyne,\",\n",
    "    ]\n",
    "\n",
    "    input_eval_lines = [\n",
    "        \"[exeunt jessica and lorenzo]\",\n",
    "        \"now, balthasar,\",\n",
    "        \"as i have ever found thee honest-true,\",\n",
    "        \"so let me find thee still. take this same letter,\",\n",
    "        \"and use thou all the endeavour of a man\",\n",
    "        \"in speed to padua: see thou render this\",\n",
    "        \"into my cousin's hand, doctor bellario;\",\n",
    "        \"and, look, what notes and garments he doth give thee,\",\n",
    "        \"bring them, i pray thee, with imagined speed\",\n",
    "        \"unto the tranect, to the common ferry\",\n",
    "        \"which trades to venice. waste no time in words,\",\n",
    "        \"but get thee gone: i shall be there before thee.\",\n",
    "        \"balthasar\\tmadam, i go with all convenient speed.\",\n",
    "        \"[exit]\",\n",
    "        \"portia\\tcome on, nerissa; i have work in hand\",\n",
    "        \"that you yet know not of: we'll see our husbands\",\n",
    "    ]\n",
    "\n",
    "    output_loop = target(\n",
    "        model,\n",
    "        data_generator,\n",
    "        batch_size=8,\n",
    "        max_length=32,\n",
    "        lines=input_lines,\n",
    "        eval_lines=input_eval_lines,\n",
    "        n_steps=1,\n",
    "        output_dir=\"test_model/\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert isinstance(output_loop, trax.supervised.training.Loop)\n",
    "        successful_cases += 1\n",
    "    except:\n",
    "        failed_cases.append(\n",
    "            {\n",
    "                \"name\": \"check_loop_type\",\n",
    "                \"expected\": trax.supervised.training.Loop,\n",
    "                \"got\": type(output_loop),\n",
    "            }\n",
    "        )\n",
    "        print(f\"Training object has the wrong type. Got {failed_cases[-1].get('got')}.\")\n",
    "\n",
    "    try:\n",
    "        strlabel = str(output_loop._tasks[0]._loss_layer)\n",
    "        assert strlabel == \"CrossEntropyLoss_in3\"\n",
    "        successful_cases += 1\n",
    "    except:\n",
    "        failed_cases.append(\n",
    "            {\n",
    "                \"name\": \"loss_layer_check\",\n",
    "                \"expected\": \"CrossEntropyLoss_in3\",\n",
    "                \"got\": strlabel,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"Wrong loss functions. CrossEntropyLoss_in3 was expected. Got {failed_cases[-1].get('got')}.\"\n",
    "        )\n",
    "\n",
    "    opt_params_dict = {\n",
    "        \"weight_decay_rate\": jnp.array(1.0e-5),\n",
    "        \"b1\": jnp.array(0.9),\n",
    "        \"b2\": jnp.array(0.999),\n",
    "        \"eps\": jnp.array(1.0e-5),\n",
    "        \"learning_rate\": jnp.array(0.0005),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        assert output_loop._tasks[0]._optimizer.opt_params == opt_params_dict\n",
    "        successful_cases += 1\n",
    "    except:\n",
    "        failed_cases.append(\n",
    "            {\n",
    "                \"name\": \"optimizer_parameters_check\",\n",
    "                \"expected\": opt_params_dict,\n",
    "                \"got\": output_loop._tasks[0]._optimizer.opt_params,\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"Wrong optimizer parameters. Expected {failed_cases[-1].get('expected')}. Got {failed_cases[-1].get('got')}.\"\n",
    "        )\n",
    "\n",
    "    # Test the optimizer parameter\n",
    "    try:\n",
    "        assert isinstance(output_loop._tasks[0].optimizer, trax.optimizers.adam.Adam)\n",
    "        successful_cases += 1\n",
    "    except:\n",
    "        failed_cases.append(\n",
    "            {\n",
    "                \"name\": \"optimizer_check\",\n",
    "                \"expected\": trax.optimizers.adam.Adam,\n",
    "                \"got\": type(output_loop._tasks[0].optimizer),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"Wrong optimizer. Expected {failed_cases[-1].get('expected')}. Got {failed_cases[-1].get('got')}.\"\n",
    "        )\n",
    "\n",
    "    # Test the metrics in the evaluation task\n",
    "    test_func = lambda x: list(map(str, x._eval_tasks[0]._metric_names))\n",
    "\n",
    "    try:\n",
    "        assert test_func(output_loop) == [\"CrossEntropyLoss\", \"Accuracy\"]\n",
    "        successful_cases += 1\n",
    "    except:\n",
    "        failed_cases.append(\n",
    "            {\n",
    "                \"name\": \"metrics_check\",\n",
    "                \"expected\": [\"CrossEntropyLoss\", \"Accuracy\"],\n",
    "                \"got\": test_func(output_loop),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"Wrong metrics in evaluations task. Expected {failed_cases[-1].get('expected')}. Got {failed_cases[-1].get('got')}.\"\n",
    "        )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bk2_unittest_test_model(target):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"check_default_example\",\n",
    "            \"input\": {\n",
    "                \"preds\": pickle.load(\n",
    "                    open(\"./test_support_files/test_preds_1.pkl\", \"rb\")\n",
    "                ),\n",
    "                \"target\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_1.pkl\", \"rb\")\n",
    "                )[1],\n",
    "            },\n",
    "            \"expected\": 1.7646706,\n",
    "        },  # 1.7646706 5.8396482\n",
    "        {\n",
    "            \"name\": \"check_example_2\",\n",
    "            \"input\": {\n",
    "                \"preds\": pickle.load(\n",
    "                    open(\"./test_support_files/test_preds_2.pkl\", \"rb\")\n",
    "                ),\n",
    "                \"target\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_2.pkl\", \"rb\")\n",
    "                )[1],\n",
    "            },\n",
    "            \"expected\": 1.5336857,\n",
    "        },  #  1.5336857 4.635229\n",
    "        {\n",
    "            \"name\": \"check_example_3\",\n",
    "            \"input\": {\n",
    "                \"preds\": pickle.load(\n",
    "                    open(\"./test_support_files/test_preds_3.pkl\", \"rb\")\n",
    "                ),\n",
    "                \"target\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_3.pkl\", \"rb\")\n",
    "                )[1],\n",
    "            },\n",
    "            \"expected\": 1.5870862,\n",
    "        },  #  1.5870862 4.889481\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        # print(\"target\", test_case[\"input\"][\"target\"])\n",
    "        # print(\"preds\", test_case[\"input\"][\"preds\"])\n",
    "        print(\"result\", result)\n",
    "        print(\"jnp.exp(result)\", jnp.exp(result))\n",
    "        print(\"np.exp(result)\", numpy.exp(result))\n",
    "\n",
    "        try:\n",
    "            assert jnp.isclose(result, test_case[\"expected\"], atol=1e-5)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your log perplexity does not match with expected value.\\nCheck if you are getting rid of the padding or checking if the target equals 0.\\nIf your result is an array instead of a float, check if you are using numpy to perform the sums.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert not jnp.exp(result) == jnp.inf\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": jnp.exp(test_case[\"expected\"]),\n",
    "                    \"got\": jnp.exp(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your perplexity overflowed. Take a look to the axis you are using in np.sum() function.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "        try:\n",
    "            assert jnp.isclose(\n",
    "                jnp.exp(result), jnp.exp(test_case[\"expected\"]), atol=1e-5\n",
    "            )\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": jnp.exp(test_case[\"expected\"]),\n",
    "                    \"got\": jnp.exp(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your perplexity does not match with expected.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unittest_test_model(target, pretrained_model):\n",
    "    successful_cases = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"check_default_example\",\n",
    "            \"input\": {\n",
    "                \"file\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_1.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 1.7646706,\n",
    "        },  # 1.7646706 5.8396482\n",
    "        {\n",
    "            \"name\": \"check_example_2\",\n",
    "            \"input\": {\n",
    "                \"file\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_2.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 1.5336857,\n",
    "        },  #  1.5336857 4.635229\n",
    "        {\n",
    "            \"name\": \"check_example_3\",\n",
    "            \"input\": {\n",
    "                \"file\": pickle.load(\n",
    "                    open(\"./test_support_files/test_batch_3.pkl\", \"rb\")\n",
    "                ),\n",
    "            },\n",
    "            \"expected\": 1.5870862,\n",
    "        },  #  1.5870862 4.889481\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        preds = pretrained_model(test_case[\"input\"][\"file\"][0])\n",
    "\n",
    "        test_case[\"input\"][\"preds\"] = preds\n",
    "        test_case[\"input\"][\"target\"] = test_case[\"input\"][\"file\"][1]\n",
    "\n",
    "        test_case[\"input\"].pop(\"file\", None)\n",
    "\n",
    "        result = target(**test_case[\"input\"])\n",
    "\n",
    "        try:\n",
    "            assert jnp.isclose(result, test_case[\"expected\"], atol=1e-5)\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": test_case[\"expected\"],\n",
    "                    \"got\": result,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your log perplexity does not match with expected value.\\nCheck if you are getting rid of the padding or checking if the target equals 0.\\nIf your result is an array instead of a float, check if you are using numpy to perform the sums.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            assert not jnp.exp(result) == jnp.inf\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": jnp.exp(test_case[\"expected\"]),\n",
    "                    \"got\": jnp.exp(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your perplexity overflowed. Take a look to the axis you are using in np.sum() function.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "        try:\n",
    "            assert jnp.isclose(\n",
    "                jnp.exp(result), jnp.exp(test_case[\"expected\"]), atol=1e-5\n",
    "            )\n",
    "            successful_cases += 1\n",
    "        except:\n",
    "            failed_cases.append(\n",
    "                {\n",
    "                    \"name\": test_case[\"name\"],\n",
    "                    \"expected\": jnp.exp(test_case[\"expected\"]),\n",
    "                    \"got\": jnp.exp(result),\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Your perplexity does not match with expected.\\n\\t Expected value near: {failed_cases[-1].get('expected')}.\\n\\t Got {failed_cases[-1].get('got')}.\"\n",
    "            )\n",
    "\n",
    "    if len(failed_cases) == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(\"\\033[92m\", successful_cases, \" Tests passed\")\n",
    "        print(\"\\033[91m\", len(failed_cases), \" Tests failed\")\n",
    "\n",
    "    # return failed_cases, len(failed_cases) + successful_cases"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
